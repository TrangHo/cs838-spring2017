{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 838 &mdash; Data Science: Principles, Algorithms, and Applications; Spring 2017 ###\n",
    "\n",
    "#  Stage 3: entity matching #\n",
    "\n",
    "#### Trang Ho, Thomas Ngo, Qinyuan Sun\n",
    "\n",
    "*****\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Introduction](#1.Introduction)\n",
    "1. [Dataset](#2.Dataset)\n",
    "1. [Training](#3.Training)\n",
    "1. [Links](#4.Links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction ##\n",
    "\n",
    "In this project stage, our team performed matching entities between two tables of education affiliations. The first table was extracted from the Academy of Management Conference (AOM) website, which contains personal information of the conference attendants in the year 2014. The personal information includes (1) individual name, (2) affiliation name, (3) country, (4) states/ province, (5) city, (6) contact numbers, (7) email address. Overall, this table consists of 9,532 entities at the individual level. \n",
    "\n",
    "The second table was extracted from the World Higher Education Database (WHED), which contains information of unique education affiliations worldwide. This table provides information on (1) affiliation name, (2) country, (3) street, (4) city, (5) province/ states, (6) postal code, (7) telephone number, and (8) website address (if available). Overall, this table consists of 17,605 unique entities at the affiliation level.\n",
    "\n",
    "In order to match individuals' affiliations on the first table to affiliations on the second table, we used their overlapped/relevant information: (1) affiliation name, (2) country, (3) province/states, (4) city, (5) website address, (6) individual email address. Our goal here is to get precision score of above 95% and recall score of as high as possible.\n",
    "\n",
    "Subsequently, we carried out the following steps using Magellan:\n",
    "* Pre-processing\n",
    "* Down-sizing the AOM table and the WHED table\n",
    "* Using a blocker to reduce the size of the potential-candidate set\n",
    "* Sampling randomly 500 pairs of potential candidates for labelling\n",
    "* Creating training and testing sets I and J\n",
    "* Training and selecting the best classifier using cross-validation\n",
    "\n",
    "More details can be found below.\n",
    "\n",
    "### Step 1. Pre-processing\n",
    "In this step, we cleaned the two datasets by standardizing information on affiliation names, country, state/province, city, email server domain. For example, we standardized states by transforming \"CA\", \"CA - California\", \"California\" to \"california\" on both the AOM table and the WHED data.\n",
    "\n",
    "### Step 2. Down-sizing\n",
    "Initially, we have 9,532 entities on the AOM table and 17,605 entities on the WHED data. After down-sizing, we have 4,000 AOM entities and 4962 WHED entities\n",
    "\n",
    "### Step 3. Blocking\n",
    "Our blocking consists of the following components:\n",
    "* Blocking all tuple pairs that have different countries\n",
    "* For American affiliations, blocking all tuple pairs that have different province/ states\n",
    "* For all affiliations, blocking all tuple pairs that have neither (1) any overlap between AOM email domain and WHED affiliation website domain nor (2) sufficient overlap coefficient (i.e. greater than 0.5) between affiliation names\n",
    "\n",
    "As a result, we reduced the size of our candidate set from 19,848,000 (=4,000 x 4,962) to 126,516. \n",
    "\n",
    "### Step 4. Sampling for labelling\n",
    "We initially sampled randomly 500 tuple pairs from the set of 126,516 potential candidates. After labeling, we dropped 22 cases due to ambiguity of the AOM information. Consequently, we had 478 tuple pairs with a density of approximately 34%.\n",
    "\n",
    "### Step 5. Creating training & testing sets\n",
    "We split the sample set into training and testing sets. As a result, each set has 239 tuple pairs.\n",
    "\n",
    "### Step 6. Training and selecting the best classifiers\n",
    "We used 6 learning methods for training on set I using cross validation. The methods include: (1) Decision Tree, (2) Random Forest, (3) SVM, (4) Naive Bayes, (5) Logistic Regression, and (6) Linear Regression. Below is the first-attempt accuracy performance of our classifiers:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Dataset ##\n",
    "\n",
    "We set aside 100 text documents as [test set](https://github.com/TrangHo/cs838-code/tree/master/test-examples) to generate testing examples and use the rest of text documents as [train set](https://github.com/TrangHo/cs838-code/tree/master/train-texts) to generate training examples.\n",
    "\n",
    "|                | Num. of documents| Num. of positive examples  | Num. of negative examples|\n",
    "| -------------  |:----------------:| :-------------:            | :-------------:          |\n",
    "| Training Set I | 200              |     725                    |  1948                    |\n",
    "| Testing Set J  | 100              |     359                    |   898                    |\n",
    "| Total          | 300              |     1084                   |  2846                    |\n",
    "\n",
    "\n",
    "\n",
    "Subsequently, we used [four main regular-expression patterns](https://github.com/TrangHo/cs838-code/blob/master/src/lib/constants/patterns.py) to create a pool of potential negative-example candidates. The patterns suggest the following characteristics of negative candidates:\n",
    "\n",
    "- having at least 2 words and all of them are capitalized\n",
    "- having 2 captialized words with a prefix of at/from/in\n",
    "- consisting of 3 or 4 words with a suffix of a noun usually goes with univerisities such as professor/student/etc.\n",
    "- consisting of 3 or words with a prefix of a verb usually goes with with universities such as attend/receive\n",
    "\n",
    "The final negative examples were then randomly selected from the pool. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Training ##\n",
    "\n",
    "To generate feature vectors from the positive and negative examples, we eventually designed 17 functions that (1) take a string and its surrounding texts, and (2) output either zero or one. Therefore our feature vector has 17 dimensions.\n",
    "\n",
    "The machine learning algoirthms we employed are: \n",
    "- Support vector machine\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- Linear regresion\n",
    "- Logistic regression \n",
    "- Multilayer perceptron neural network\n",
    "\n",
    "\n",
    "We initially had only 16 features. The average precision and recall of 5-fold cross-validation are listed as follow. However, the results of our classifiers were close but did not meet the requirement of having (1) precision of 90% or higher and (2) recall of 50% or higher. After inspecting the false positives and false negatives, we found out that a prevalent problem was that single-word university names (such as Yale, Standford, and Columbia) were wrongly classifed as negatives. As a result, we added a dictionary of short names for popular universities for these case as feature 17. This feature significantly increases both precisons and recalls of all classifiers.\n",
    "\n",
    "__Precision & Recall with 16 Features__\n",
    "\n",
    "| Machine Learning Algorithm| Ave CV Precision | Ave CV Recall  |     F1    |\n",
    "| ------------------------- |:----------------:| :-------------:|:---------:|\n",
    "| Support Vector Machine    | 0.92             |     0.49       | 0.64      |\n",
    "| Decsion Tree              | 0.89             |     0.54       | 0.67      | \n",
    "| Random Forest             | 0.89             |     0.54       | 0.67      | \n",
    "| Logistic Regression       | 0.90             |     0.50       | 0.64      | \n",
    "| Neural Network            | 0.88             |     0.54       | 0.67      | \n",
    "\n",
    "__Precision & Recall with 17 Features__\n",
    "\n",
    "| Machine Learning Algorithm| Ave CV Precision | Ave CV Recall  |     F1    |\n",
    "| ------------------------- |:----------------:| :-------------:|:---------:|\n",
    "| Support Vector Machine    | 0.95             |     0.70       | 0.81      |\n",
    "| Decsion Tree              | 0.93             |     0.72       | 0.81      |\n",
    "| Random Forest             | 0.92             |     0.74       | 0.82      |\n",
    "| Linear Regression         | 0.97             |     0.67       | 0.79      |\n",
    "| Logistic Regression       | 0.95             |     0.70       | 0.81      |\n",
    "| Neural Network            | 0.92             |     0.73       | 0.81      |\n",
    "\n",
    "We chose Support Vector Machine as our classifier. We trained the classifier with all the training examples and tested on the testing examples. The results are shown in the following table.\n",
    "\n",
    "|Type  |Precision| Recall |F1  |\n",
    "| ---- |:-------:|:------:|:--:|\n",
    "|TRAIN |0.93     | 0.73   |0.82|\n",
    "|TEST  |0.97     | 0.72   |0.83|\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Links ##\n",
    "\n",
    "[link](https://github.com/TrangHo/cs838-code/tree/master/texts) to 300 text document\n",
    "\n",
    "[link](https://github.com/TrangHo/cs838-code/tree/master/train-texts) to training set\n",
    "\n",
    "[link](https://github.com/TrangHo/cs838-code/tree/master/test-examples) to test set\n",
    "\n",
    "[link](https://github.com/TrangHo/cs838-code/tree/master/src) to source code\n",
    "\n",
    "[link](https://github.com/TrangHo/cs838-spring2017/raw/master/cs838-stage2.zip) to a zip file for stage 2 related documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
