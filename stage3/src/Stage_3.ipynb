{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magellan version:0.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import py_entitymatching as em\n",
    "print('magellan version:' + em.__version__)\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/carepjan/code/website/stage3/csv_files/'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = os.path.dirname(os.getcwd())\n",
    "path_to_csv_dir = working_dir + os.sep + 'csv_files'+ os.sep\n",
    "path_to_csv_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 - PRE-PROCESSING DATA\n",
    "\n",
    "In this stage, we need to preprocess data before applying Megellan. This is because our datasets (especially the AOM dataset) are quite dirty, and therefore adversely affecting Megellan's blocking and matching functions. For example, states can take any value of \"CA\", \"California\", or \"CA - California\".\n",
    "\n",
    "In this step, we will clean the following variables:\n",
    "* Country name (e.g. Whed data has 2 Belgiums: (1) Belgium - French Community and (2) Belgium - Flemish Community)\n",
    "* State name\n",
    "* City name\n",
    "* Affiliation name\n",
    "* Email server domain (we will only capture the university information from the email server domain - if there is any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_domains = [\"ac\",\"ad\",\"ae\",\"af\",\"ag\",\"ai\",\"al\",\"am\",\"an\",\"ap\",\"aq\",\"ar\",\"as\",\"at\",\"au\",\n",
    "                  \"az\",\"ba\",\"bb\",\"be\",\"bf\",\"bg\",\"bh\",\"bi\",\"bm\",\"bn\",\"bo\",\"br\",\"bt\",\"by\",\"bz\",\n",
    "                  \"ca\",\"cc\",\"cd\",\"cf\",\"cg\",\"ch\",\"ck\",\"cl\",\"cm\",\"cn\",\"co\",\"cr\",\"cu\",\"cx\",\"cy\",\n",
    "                  \"cz\",\"de\",\"dj\",\"dk\",\"do\",\"dz\",\"ec\",\"ee\",\"eg\",\"es\",\"fi\",\"fj\",\"fk\",\"fm\",\"fr\",\n",
    "                  \"fo\",\"gb\",\"ge\",\"gf\",\"gg\",\"gh\",\"gi\",\"gl\",\"gm\",\"gn\",\"gr\",\"gs\",\"gt\",\"gu\",\"hk\",\n",
    "                  \"hm\",\"hn\",\"hr\",\"hu\",\"id\",\"ie\",\"il\",\"im\",\"in\",\"int\",\"io\",\"ir\",\"is\",\"it\",\"je\",\n",
    "                  \"jo\",\"jp\",\"ke\",\"kg\",\"kh\",\"kr\",\"kw\",\"ky\",\"kz\",\"lb\",\"lc\",\"li\",\"lk\",\"lr\",\"lt\",\n",
    "                  \"lu\",\"lv\",\"ly\",\"mc\",\"md\",\"mg\",\"mh\",\"mk\",\"mm\",\"mn\",\"mo\",\"mp\",\"mq\",\"mr\",\"ms\",\n",
    "                  \"mt\",\"mu\",\"mx\",\"my\",\"mw\",\"na\",\"nc\",\"nf\",\"ni\",\"nl\",\"no\",\"np\",\"nu\",\"nz\",\"om\",\n",
    "                  \"pa\",\"pe\",\"pg\",\"ph\",\"pk\",\"qa\",\"re\",\"ro\",\"ru\",\"rw\",\"sa\",\"sb\",\"se\",\"sg\",\"sh\",\n",
    "                  \"si\",\"sk\",\"sm\",\"sn\",\"so\",\"st\",\"su\",\"sv\",\"sz\",\"tc\",\"td\",\"tf\",\"th\",\"tj\",\"tm\",\n",
    "                  \"tn\",\"to\",\"tp\",\"tr\",\"tt\",\"tv\",\"tw\",\"tz\",\"ua\",\"ug\",\"uk\",\"um\",\"us\",\"uy\",\"uz\",\n",
    "                  \"ve\",\"vg\",\"vi\",\"vu\",\"wf\",\"ws\",\"yt\",\"yu\",\"za\",\"zm\"]\n",
    "academic_domains = [\"edu\", \"ac\"]\n",
    "common_domains = [\"net\",\"com\",\"info\",\"org\"]\n",
    "country_dict = {        \n",
    "    \"Belgium - Flemish Community\": \"belgium\",\n",
    "    \"Belgium - French Community\": \"belgium\",\n",
    "    \"Bolivia (Plurinational State of)\": \"bolivia\",\n",
    "    \"China - Hong Kong SAR\": \"hong kong\",\n",
    "    \"China - Macao SAR\": \"macau\",\n",
    "    \"China - Taiwan\": \"taiwan\",\n",
    "    \"Congo (Democratic Republic)\": \"congo\",\n",
    "    \"France - French Guyana\": \"france\",\n",
    "    \"France - French Polynesia\": \"france\",\n",
    "    \"France - Guadeloupe\": \"france\",\n",
    "    \"France - Martinique\": \"france\",\n",
    "    \"France - New Caledonia\": \"france\",\n",
    "    \"France - Reunion\": \"france\",\n",
    "    \"Gambia (The)\": \"gambia\",\n",
    "    \"Iran (Islamic Republic of)\": \"iran\",\n",
    "    \"Korea (Democratic People's Republic of)\": \"north korea\",\n",
    "    \"KOREA, REPUBLIC OF\": \"south korea\",\n",
    "    \"Lao People's Democratic Republic\": \"laos\",\n",
    "    \"Macedonia (The Former Yugoslav Republic)\": \"macedonia\",\n",
    "    \"TRINIDAD\": \"trinidad and tobago\",\n",
    "    \"United States of America \": \"united states\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"venezuela\",\n",
    "    \"Korea (Republic of)\": \"south korea\"\n",
    "}  \n",
    "\n",
    "us_states = {\n",
    "  \"AB\": { \"country\": \"canada\", \"province\": \"Alberta\" },\n",
    "  \"AK\": \"Alaska\",\n",
    "  \"AL\": \"Alabama\",\n",
    "  \"AR\": \"Arkansas\",\n",
    "  \"AZ\": \"Arizona\",\n",
    "  \"Baltimore\": \"Maryland\",\n",
    "  \"Brighton\": \"Colorado\",\n",
    "  \"CA\": \"California\",\n",
    "  \"CA - California\": \"California\",\n",
    "  \"California(CA)\": \"California\",\n",
    "  \"CO\": \"Colorado\",\n",
    "  \"CT\": \"Connecticut\",\n",
    "  \"D.C.\": \"District of Columbia\",\n",
    "  \"DC\": \"District of Columbia\",\n",
    "  \"DE\": \"Delaware\",\n",
    "  \"District of Col\": \"District of Columbia\",\n",
    "  \"FL\": \"Florida\",\n",
    "  \"GA\": \"Georgia\",\n",
    "  \"HI\": \"Hawaii\",\n",
    "  \"IA\": \"Iowa\",\n",
    "  \"ID\": \"Idaho\",\n",
    "  \"IL\": \"Illinois\",\n",
    "  \"IN\": \"Indiana\",\n",
    "  \"KS\": \"Kansas\",\n",
    "  \"KY\": \"Kentucky\",\n",
    "  \"LA\": \"Louisiana\",\n",
    "  \"Lisbon\": { \"country\": \"portugal\", \"province\": \"Lisbon\"},\n",
    "  \"M0\": \"Missouri\",\n",
    "  \"MA\": \"Massachusetts\",\n",
    "  \"MA - Massachuse\": \"Massachusetts\",\n",
    "  \"MA.\": \"Massachusetts\",\n",
    "  \"Mass.\": \"Massachusetts\",\n",
    "  \"MD\": \"Maryland\",\n",
    "  \"MD - Maryland\": \"Maryland\",\n",
    "  \"ME\": \"Maine\",\n",
    "  \"MI\": \"Michigan\",\n",
    "  \"MI.\": \"Michigan\",\n",
    "  \"Minn\": \"Minnesota\",\n",
    "  \"Minnesota\": \"Minnesota\",\n",
    "  \"MN\": \"Minnesota\",\n",
    "  \"MO\": \"Missouri\",\n",
    "  \"MS\": \"Mississippi\",\n",
    "  \"MT\": \"Montana\",\n",
    "  \"NC\": \"North Carolina\",\n",
    "  \"ND\": \"North Dakota\",\n",
    "  \"NE\": \"Nebraska\",\n",
    "  \"NH\": \"New Hampshire\",\n",
    "  \"NJ\": \"New Jersey\",\n",
    "  \"NM\": \"New Mexico\",\n",
    "  \"NV\": \"Nevada\",\n",
    "  \"NY\": \"New York\",\n",
    "  \"OH\": \"Ohio\",\n",
    "  \"OK\": \"Oklahoma\",\n",
    "  \"OR\": \"Oregon\",\n",
    "  \"PA\": \"Pennsylvania\",\n",
    "  \"Penn\": \"Pennsylvania\",\n",
    "  \"RI\": \"Rhode Island\",\n",
    "  \"SC\": \"South Carolina\",\n",
    "  \"SD\": \"South Dakota\",\n",
    "  \"TN\": \"Tennessee\",\n",
    "  \"TX\": \"Texas\",\n",
    "  \"TX - Texas\": \"Texas\",\n",
    "  \"UT\": \"Utah\",\n",
    "  \"VA\": \"Virginia\",\n",
    "  \"VT\": \"Vermont\",\n",
    "  \"WA\": \"Washington\",\n",
    "  \"WI\": \"Wisconsin\",\n",
    "  \"wisconsin\": \"Wisconsin\",\n",
    "  \"WV\": \"West Virginia\",\n",
    "  \"WY\": \"Wyoming\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A. Clean AOM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_name</th>\n",
       "      <th>a_city</th>\n",
       "      <th>a_prov</th>\n",
       "      <th>a_country</th>\n",
       "      <th>a_email_server</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academy management</td>\n",
       "      <td>briarcliff manor</td>\n",
       "      <td>new york</td>\n",
       "      <td>united states</td>\n",
       "      <td>aom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>northeastern university</td>\n",
       "      <td>boston</td>\n",
       "      <td>massachusetts</td>\n",
       "      <td>united states</td>\n",
       "      <td>gmail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>skidmore college</td>\n",
       "      <td>saratoga springs</td>\n",
       "      <td>new york</td>\n",
       "      <td>united states</td>\n",
       "      <td>skidmore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            a_name            a_city         a_prov  \\\n",
       "person_id                                                             \n",
       "1               academy management  briarcliff manor       new york   \n",
       "4          northeastern university            boston  massachusetts   \n",
       "5                 skidmore college  saratoga springs       new york   \n",
       "\n",
       "               a_country a_email_server  \n",
       "person_id                                \n",
       "1          united states            aom  \n",
       "4          united states          gmail  \n",
       "5          united states       skidmore  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path_to_csv_dir + '_aom.csv', encoding = 'UTF-8', index_col = 'person_id')\n",
    "for i in ['full_name','first_name','last_name','phone','fax','email','person_html_name']:\n",
    "    del data[i]\n",
    "data.head(n=3)\n",
    "\n",
    "### CLEAN AFFILIATION INFORMATION\n",
    "def aomclean_affiliation(a_name):\n",
    "    a_name = a_name.str.replace('U.','University')\n",
    "\n",
    "    to_remove = [' of ', ' at ', ' in ',',', '-', '&', '(', ')', r'\\s+']\n",
    "    for i in to_remove:\n",
    "        a_name = a_name.str.replace(i, \" \")\n",
    "\n",
    "    a_name = a_name.str.replace(\"'\",'')\n",
    "    a_name = a_name.str.lower()\n",
    "    return a_name\n",
    "\n",
    "### CLEAN EMAIL SERVER INFORMATION\n",
    "def aomclean_email_server(a_email_server):\n",
    "    # This function aims to return the essential/ importantant information from individuals' email\n",
    "    # server domain. For example, in the case of email server of \"wharton.upenn.edu\" and university \n",
    "    # website address of \"http://www.upenn.edu\", important is the information \"upenn\". This is\n",
    "    # because we want to map affiliations at the university level, instead of the school level,\n",
    "    #  e.g. Wharton is the business school of University of Pensylvenia. Subsequently, a feature will\n",
    "    #  capture whether the essential information is contained in the affiliation's website address\n",
    "\n",
    "    # In order to extract the important information, we eliminate (1) country domains (e.g. \"au\" is\n",
    "    # Australia's country domain), (2) academic domains (i.e. \"edu\" and \"ac\" )), and (3) common domains\n",
    "    # (e.g. \"com, \"net\", \"info\"). After eliminating these domains, we take the first info - which\n",
    "    # is supposed to capture the highest level of domain at the affiliation level. For example, in\n",
    "    # the case of \"wharton.upenn.edu\", after eliminating the abovementioned domains, we will have\n",
    "    # \"wharton.upenn\", of which \"upenn\" is at the university level and \"wharton\" is at the school\n",
    "    # level. We will only capture the university-evel info, i.e. \"upenn\"\n",
    "\n",
    "\n",
    "    # EXAMPLE: \n",
    "    # \"Catolica Lisbon School of Business and Economics\" - is the business school of Catholic U. Portugal\n",
    "    # AOM's email server domain: \"clsbe.lisboa.ucp.pt\" ; WHED's affiliation website: \"http://www.ucp.pt\"\n",
    "    # -----\n",
    "    # The essential information of the email server domain is \"ucp\"\n",
    "    # \"ucp\" appears in the affiation website\n",
    "    # --> This helps link school to university !!!!!!!! YAYYYYYYY\n",
    "    global country_domain \n",
    "    global academic_domains \n",
    "    global common_domains\n",
    "\n",
    "    n = a_email_server.size\n",
    "    for i in range (0, n):\n",
    "        email_server = a_email_server.iloc[i].strip().split('.')\n",
    "\n",
    "        # The sequence of the following conditional commands is important. It is because the country\n",
    "        # domains are usually the last one on the right of an email server domain, and it is followed\n",
    "        # either by academic domain and/or common domain. \n",
    "        if email_server[-1] in country_domains:\n",
    "            del email_server[len(email_server)-1]\n",
    "\n",
    "        if email_server[-1] in academic_domains:\n",
    "            del email_server[len(email_server)-1]\n",
    "        if email_server[-1] in common_domains:\n",
    "            del email_server[len(email_server)-1]\n",
    "        if email_server[-1] in academic_domains:\n",
    "            del email_server[len(email_server)-1]\n",
    "\n",
    "        # return the highest level information only\n",
    "        a_email_server.iloc[i] = email_server[len(email_server)-1]\n",
    "    return a_email_server\n",
    "\n",
    "### CLEAN CITY INFORMATION\n",
    "def aomclean_city(a_city):\n",
    "    df = pd.read_csv(path_to_csv_dir + \"aom_list.csv\")\n",
    "    inputs = df['Input'].values\n",
    "    outputs = df['Output'].values\n",
    "    D = dict(zip(inputs,outputs))\n",
    "\n",
    "    n = a_city.size\n",
    "    for i in range (0, n):\n",
    "        if a_city.iloc[i] in D:\n",
    "            a_city.iloc[i] = D[a_city.iloc[i]]\n",
    "            \n",
    "    to_remove = [' of ', ' at ', ' in ',',','.', '-', '&', '(', ')',r'\\s+' ]\n",
    "    for i in to_remove:\n",
    "        a_city = a_city.str.replace(i, \" \")\n",
    "    a_city = a_city.str.replace(\"'\",'')\n",
    "    a_city = a_city.str.lower()        \n",
    "            \n",
    "    return a_city\n",
    "\n",
    "### CLEAN COUNTRY INFORMATION\n",
    "def aomclean_country(a_country):\n",
    "    global country_dict\n",
    "    \n",
    "    n = a_country.size\n",
    "    for i in range (0, n):\n",
    "        if a_country.iloc[i] in country_dict:\n",
    "            a_country.iloc[i] = country_dict[a_country.iloc[i]]\n",
    "    a_country = a_country.str.lower()\n",
    "    a_country = a_country.str.strip()\n",
    "    return a_country\n",
    "\n",
    "### CLEAN STATES INFORMATION\n",
    "def aomclean_states(a_prov, a_country):\n",
    "    global us_states\n",
    "    \n",
    "    n = a_prov.size\n",
    "    for i in range (0, n):\n",
    "        if a_prov.iloc[i] in us_states:\n",
    "            if isinstance(us_states[a_prov.iloc[i]], str):\n",
    "                a_prov.iloc[i] = us_states[a_prov.iloc[i]]\n",
    "            else:\n",
    "                a_country.iloc[i] = us_states[a_prov.iloc[i]][\"country\"]\n",
    "                a_prov.iloc[i] = us_states[a_prov.iloc[i]][\"province\"]                \n",
    "    a_prov = a_prov.str.lower()\n",
    "    a_prov = a_prov.str.strip()\n",
    "    return (a_prov, a_country)\n",
    "\n",
    "### --------------------------------------------------------\n",
    "### CLEANING\n",
    "### --------------------------------------------------------\n",
    "data.a_name = aomclean_affiliation(data.a_name)\n",
    "data.a_email_server = aomclean_email_server(data.a_email_server)\n",
    "data.a_city = aomclean_city(data.a_city)\n",
    "data.a_country = aomclean_country(data.a_country)\n",
    "data.a_prov, data.a_country = aomclean_states(data.a_prov, data.a_country)\n",
    "\n",
    "### SAVE AS A CLEANED CSV\n",
    "data.to_csv(path_to_csv_dir + '_aom_cleaned.csv', encoding = 'UTF-8', index_label = 'person_id')\n",
    "data.head(n=3)\n",
    "# data.a_country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B. Clean WHED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_name</th>\n",
       "      <th>a_country</th>\n",
       "      <th>a_city</th>\n",
       "      <th>a_prov</th>\n",
       "      <th>a_web</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pampanga state agricultural university</td>\n",
       "      <td>philippines</td>\n",
       "      <td>magalang</td>\n",
       "      <td>Pampanga</td>\n",
       "      <td>http://www.pac.edu.ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>les roches international school hotel management</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>bluche crans montana</td>\n",
       "      <td>Bluche-Crans-Montana</td>\n",
       "      <td>http://www.lesroches.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dharma gate budapest buddhist university</td>\n",
       "      <td>hungary</td>\n",
       "      <td>budapest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.tkbf.eu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                a_name    a_country  \\\n",
       "a_id                                                                  \n",
       "2               pampanga state agricultural university  philippines   \n",
       "4     les roches international school hotel management  switzerland   \n",
       "6             dharma gate budapest buddhist university      hungary   \n",
       "\n",
       "                    a_city                a_prov                     a_web  \n",
       "a_id                                                                        \n",
       "2                 magalang              Pampanga     http://www.pac.edu.ph  \n",
       "4     bluche crans montana  Bluche-Crans-Montana  http://www.lesroches.edu  \n",
       "6                 budapest                   NaN        http://www.tkbf.eu  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path_to_csv_dir + '_whed.csv', encoding = 'ISO-8859-1', index_col = 'a_id')\n",
    "for i in ['a_street','a_pcode','a_tel']:\n",
    "    del data[i]\n",
    "data.head(n=3)\n",
    "\n",
    "### CLEAN AFFILIATION INFORMATION\n",
    "def whedclean_affiliation(a_name):\n",
    "    a_name = a_name.str.replace('U.','University')\n",
    "\n",
    "    to_remove = [' of ', ' at ', ' in ',',', '-', '&', '(', ')', r'\\s+']\n",
    "    for i in to_remove:\n",
    "        a_name = a_name.str.replace(i, \" \")\n",
    "\n",
    "    a_name = a_name.str.replace(\"'\",'')\n",
    "    a_name = a_name.str.lower()\n",
    "    return a_name\n",
    "\n",
    "### CLEAN CITY INFORMATION\n",
    "def whedclean_city(a_city):\n",
    "    to_remove = [' of ', ' at ', ' in ',',','.', '-', '&', '(', ')',r'\\s+' ]\n",
    "    for i in to_remove:\n",
    "        a_city = a_city.str.replace(i, \" \")\n",
    "    a_city = a_city.str.replace(\"'\",'')\n",
    "    a_city = a_city.str.lower()          \n",
    "    return a_city\n",
    "\n",
    "### CLEAN COUNTRY INFORMATION\n",
    "def whedclean_country(a_country):\n",
    "    global country_dict\n",
    "    \n",
    "    n = a_country.size\n",
    "    for i in range (0, n):\n",
    "        if a_country.iloc[i] in country_dict:\n",
    "            a_country.iloc[i] = country_dict[a_country.iloc[i]]\n",
    "    a_country = a_country.str.lower()\n",
    "    a_country = a_country.str.strip()\n",
    "    \n",
    "    return a_country\n",
    "\n",
    "### --------------------------------------------------------\n",
    "### CLEANING\n",
    "### --------------------------------------------------------\n",
    "data.a_name = whedclean_affiliation(data.a_name)\n",
    "data.a_city = whedclean_city(data.a_city)\n",
    "data.a_country = whedclean_country(data.a_country)\n",
    "\n",
    "### SAVE AS A CLEANED CSV\n",
    "data.to_csv(path_to_csv_dir + '_whed_cleaned.csv', encoding = 'UTF-8', index_label = 'a_id')\n",
    "data.head(n=3)\n",
    "# data.a_country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 - MAGELLAN - BLOCKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 13: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3033fb7c32d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_to_csv_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/thomasngo/Google Drive/_UW Madison/16-17_S4/CS 838/_Project/Stage3/csv/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_csv_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_aom.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'person_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(em.get_key(A))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_csv_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_whed.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py_entitymatching/io/parsers.py\u001b[0m in \u001b[0;36mread_csv_metadata\u001b[0;34m(file_path, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Read the csv file using pandas read_csv method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Get the value for 'key' property and update the catalog.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:10415)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:10691)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:11728)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_column_data (pandas/parser.c:13162)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_tokens (pandas/parser.c:14116)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_with_dtype (pandas/parser.c:16172)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._string_convert (pandas/parser.c:16400)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser._string_box_utf8 (pandas/parser.c:22072)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 13: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "A = em.read_csv_metadata(path_to_csv_dir + '_aom.csv', key = 'person_id')\n",
    "print(em.get_key(A))\n",
    "\n",
    "B = em.read_csv_metadata(path_to_csv_dir + '_whed.csv', key = 'a_id')\n",
    "print(em.get_key(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telfer school management university ottawa\n"
     ]
    }
   ],
   "source": [
    "sample_A, sample_B = em.down_sample(A, B, size=500, y_param=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
